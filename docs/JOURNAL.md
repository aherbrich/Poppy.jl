# Journal

## 29th April - 5th May
This week I implemented a psuedolegal move generator. This means that moves are only filtered for legality after generation. The idea behind this approach is the following: In an optimal search, only few moves are searched at each level. It should therefore be more efficient to check for legality only for the moves that are actually searched, rather than for all moves in advance.   

The board is represented in a hybrid way, using a bitboard for the pieces and an additional array for the piece types. This allows for efficient calculation, as the bitboard can be used for fast move generation (bitshifts etc.) and the array for fast piece lookup (indexing). The downside is that one has to keep an additional data structure (the array) in sync with the (already sufficient) bitboards, which will lead to additional comptuation overhead. However, this overhead should be negligible compared to the time saved by faster piece lookup.

The move generator uses magic bitboards for sliding pieces (rooks, bishops, queens). For the other pieces (pawns, knights, kings), the moves are precalculated and stored in lookup tables. This allows for fast move generation, as the moves (for each piece type) can be looked up directly from the tables, and only need to be filtered for legality afterwards. The downside is that the tables take up additional memory, but this should be negligible compared to the time saved by faster move generation.

The move generator is implemented in a modular way, with function overloading on the color of the pieces. This allows for a more readable and maintainable codebase, as the move generation for white and black pieces can be implemented separately. Additionaly, one can reduce the amount of if-else statements in the code, as the color of the pieces is passed as an argument to the function.

The move generator is tested using the Perft algorithm, which calculates the number of possible moves for a given position. The results are compared to the expected values which were calculated using a reference implementation. 

## 6th May - 12th May
This week was all about optimizing the move generator. 

After careful consideration, I decided to switch to a legal move generator. The reason for this is that this improves the readability of the code and reduces the risk of bugs. Especially in the context of pattern learning, where move generation will play a central role, the overhead of legality check is negligible compared to the additional complexity of legality filtering at the correct points in the code.

The 'legal' in legal move generator refers to the fact that only legal moves are generated (not that generated psuedolegal moves are filtered for legailty). This requires additional computation upfront, but reduces the amout of memory allocations and frees considerably. For this reason, the legal move generator is actually faster than the psuedolegal move generator.

After profiling the code, I identified that a lot of time was spent in julias multiple dispatch system and there was quite a lot of function call overhead. I therefore removed the function overloading and inlined most of the code. This led to a significant speedup, as the compiler was able to optimize the code more effectively.

Additionally, I refactored the code for better readability and setup CI/CD using Github Actions. This ensures that the code is tested automatically after each commit and ensures the correctness of the move generator for potential future changes.

## 13th May - 19th May
This week two main (complety unrelated) features were implemented:

On the one hand, I implemented a very simple command line chess tournament manager. The code is in a different repository, as it is not directly related to the chess engine. The tournament manager uses the already implemented chess engine in it's backend. In the context of the tournament manager, the engine was extended by several functions such as move extraction by UCI move strings.

On the other hand, for the pattern learning based on factor graphs and the sum-product algorithm, I implemented the necessary data structures and functions.

## 20th May - 26th May
This week a lot happend.

First, I added a very simple search to the engine. The search is a alpha-beta search in an iterative deepening framework. The search is very basic and only uses the material difference as the evaluation function. The search is not optimized and is only used for testing purposes. In this context I also added threefold repetition detection and fifty move rule detection. Additionaly a transposition table was added to the search, and in this context (incremental) Zobrist hashing was implemented. This slows down the actual move generation (since the hash is updated on every do and undo move), but (under the premise that we want a transposition table) is still the fastest way. 

Second, a simple UCI interface was added to the engine. The UCI interface is still very basic but enables the engine to be used with any UCI compatible GUI. The UCI interface is tested using the own and c-chess-cli tournament manager.

Lastly, I trained my first model using the factor graph based pattern learning approach. The model was trained on a dataset of roughly 20000 games (with an ELO of 2400+) collected from the Lichess database. The idea behind the model is as follows: We make the assumpton that for every position (that occured in a game) in the dataset, the best move for that position was played by the expert. We interpret this as a ranking problem. For a given set of possible moves, the player at turn ranked the move played higher than all other moves. We call the value by which this ranking happend the 'urgency' of a move. The urgency is represented by a Gaussian distribution, which is parameterized by the mean and variance. The mean is the value of the urgency and the variance is the uncertainty over the urgency.

Now we build following factor graph:
- For every played move and possible (but not played) move, we add node. We call these nodes the 'urgency' nodes. This node type is connected to a prior factor, namely a gaussian distrubtion. We use mean 0 and variance 1 for the prior. The values we choose here are (more or less) arbitrary, since we are interested in the relative values of the urgencies. These are the nodes over which value we want to infer (we want to learn the urgencies). A move is mapped to its corresponding node by an index given by (source, dest, piece type = 64 x 64 x 12).
- For every position (we encounter while training) we add a node for every possible move of that position. We call these nodes 'latent' nodes. They are connected to the corresponding 'urgency' nodes by a gaussian mean factor. The value of such a 'latent' node is represented by a gaussian distribution, with mean equal to the urgency of the 'urgency node' it is connected to. The variance is some arbitary value. The idea of this node is the following: Each urgency node has some underlying "true" urgency. The urgency the expert assigned to the move is a noisy version of this true urgency. It is somewhere around the true urgency, but not exactly. This accounts for multiple factors: On the one hand, in some cases the expert will not have actually played the best move. On the other hand, in somes positions (especially in the opening) multiple moves are 'equally' good. In this case, the expert will have chosen one of them, and the model accounts for this by letting similarly urgent moves to have some 'wiggle room' in there urgencies. Moreover, the latent node account for the fact that the encoding of the move is not perfect. For example the move (e2, e4, pawn) is possible on multiple boards, but irrespective of this, two such move, map to the same urgency node (even though they were played in different contexts). This is a big flaw of the model, since moves (and the corresponding urgency nodes) have no context encoded into them. The latent node accounts for this by adding some noise to the urgency. 
- For every position (we encounter while training) we sort the 'latent' nodes in such a way that the 'latent' node of the move played by the expert is the first node. Then we add a so called 'difference' node (linked by a difference factor)  with the 1st node and every other node (example: node 1 and 2 are linked with node d_1,2, 1 and 3 are linked with node d_1,3 etc.). Like the names suggest, this nodes measures the difference in urgency between the first node and every other node. This value of this node is (again) represented by a gaussian distribution, i.e. we have a probability distribution for every possible difference value.
- Lastly, we add a 'greater-than' node for every 'difference' node. This node is linked by a greater-than factor to the 'difference' node. The idea is that the difference in urgency between the first node and every other node is assumed to be positive.

One can now learn the urgencies by inference over the factor graph. The sum-product algorithm is used for this. The algorithm is an iterative message passing algorithm, which passes messages between the nodes of the factor graph.

The model has an accuracy of roughly 25% (on a test dataset), but still performs poorly in actual play. 25% might not seem that high, but is actually quite good if one considers that it has to predict out of 30 moves on average and that in many positions, experts frequently play multiple 'equally' good moves. If one would use a determistic choice of move for a given position, as one could expect from a chess engine like stockfish (for a given depth), the accuracy would probably be much higher.


## 27th May - 2nd June
This week the model was redisgned. The big flaw of the previous model was, that it learned the value of a move, irrespective of the context in which the move was played. This led to a lot of noise in the model, since the same move could occur in multiple contexts. In hindsight, it is actually quite astonishing that the model performed as well as it did, given this flaw.  The idea of the new model is as follows: Instead of learning move urgencies, we learn state/board urgencies. For this we actually play the played move (and all the other possible moves) and learn the value of the resulting board, by ranking the boards. This leads to learning urgencies of boards, which has the additional benefit, that one could use the urgencies in the evaluation function of the search, though it is still unclear how this could be done, or if the eval is well enough calibrated (since we have this weird relative value property).

Of course it is unfeasible to learn the value of every possible board (since there are way to many), so we have to find a way to reduce the number of boards we have to learn. The idea is to view a board as a set of features. In the new model we assume the board urgency to be the sum of the urgencies of the features of the board. For wisely chosen features, this leads to a much smaller number of urgencies we have to learn (at most number of features many). The features where chosen as follows: Every move (that can be played on a given board) is a feature. Since this (most likely) generalizes way to much (like the last model), we add the following features: If k many moves were possible, we add a "double" feature for every pair of moves (for all i, j with 1 <= i < j <= k). The idea behind this is that in many cases, the co or non-occurence of two moves is more or less significant for the urgency of the board. At least in theory, one can view the "single" features as a sort of baseline urgency of the board, and the "double" features as a sort of correction to this baseline. The model is trained using the similar factor graph design as before, but between the urgency nodes and laten nodes, we add a sum node. This sum node is the urgency of the board. The model is tested on the same dataset as before. There are atmost 64x64x12 (=49.152) many "single" features and 64x64x12x64x64x12 (=2.415.919.104) many "double" features. On first glance, this is still way to much, but we make follwing interesting observation: In the dataset, only about 8000 "single" features actually occur (since some are just never played). For the "double" features the effect is even more pronounced, since "only" a couple of million "double" features actually occur.

The new model has an accuracy of roughly 30% (on a test dataset), but still performs poorly in actual play. Additionaly to actually rank moves against eachother is way slower, since every move has to be played, the boards features extracted, summed and ranked. This raises following question: For future models, how should one prioritize between accuracy and speed? 

In the context of actually implementing and learning the model many problems with float precision occured. The factors of the factor graph were updated (where possible) to work in natural parameter space (instead of mean and variance). The advantage with Gaussians in natural parameter space is that they are more stable, since multiplicationa and divison turn into addition and subtraction. This is especially important in the context of the sum-product algorithm, where many multiplications and divisions are performed.

## 3rd June - 9th June
For future model improvements metadata and analysis tools were added. This should simplify model testing.

This week was all about designing a new and better model. I reallly liked the idea of learning board urgencies so the general idea stays the same. However, the features were redesigned. It seems plausible that if one could also use 'triplet' (and 'quadruple', 'fivetuple', ...) features the model would be more accurate. This follows the reasoning that currently some features appear in many boards, also those that are in no way related to each other. This still leads to a lot of noise in the model. By increasing the "size" of the features, we can reduce the noise (since bigger features, or what one could view as a pattern, match to less boards, namely only to those that are more 'similar' to each other). In the best case a feature/pattern is maximal, i.e. it only matches to one board, which is the case is if the pattern is the combination of all possible moves. Unfortunaly, one can not simply build all combination of features for a board, since the number of features would explode. However, we can try to find a subset of patterns, namely those that are the most relevant for the position. These features will likely be very large for opening positions (since here many single features often cooccur) and will gradually be smaller the further the game progresses. This is closly linked to how humans parse a chess position, i.e. they chunk the board into meaningful parts. By this, experts can efficiently recognize patterns (and more or less unconsciously) compare this to their 'knowledge' base. This works very weel in the opening and less well in the endgame. 

This weeks challenge was about finding a good set of meaningful (in size varying) features, that is small enough to be learned, but big enough to be accurate. For boards which can be broken down into big features (i.e. opening positions), the model should then be able to predict the best move with high accuracy. For boards which can only be broken down into small (maybe even minmal) features (i.e. endgame positions), the model will likely predict the best move with less accuracy. Atleast the value of the a feature (also the small ones) should be more accurate now, since they will only be learnt from positions where one couldnt match a bigger feature.

How does one find a good set of features? I don't know. I experimented with counting bloom filters. The idea was to find the N most frequent single features and then add all possible double features from that set of N features. Then again to reduce thise double features to the N most frequent etc. Unforunaltely this still generates way to many candidate features.

## 10th June - 16th June & 17th June - 23rd June
The last two weeks I explored the world of frequent pattern mining. The idea of frequent pattern mining is the following. One has a database of transactions (transaction = a set of items), like Amazons database of all customer purchases. One now wants to find those items that are most often bought together, like Amazons "customers also bought". The same concepts and algorithms can be applied to features of boards. On chooses the "single" features (at most 64x64x12 many, in practice ~8000) as the items. Now one finds those combinations of features (items) that are most frequent accross all boards (transactions) in the database.

Currently, the most promising idea is the FP-Tree data structure and it's FP-Growth mining algorithm. The FP-Tree is a prefix tree. The idea is to scan the database once, and sort the single features by number of occurences. Now on second database scan, one builds the FP-Tree. For every board one generates the features of the board. The features are then sorted by number of occurences. Features that dont meet a minimum threshold are filtered (since all combinations with that feature would also not meet the threshold). The features are then added to the FP-Tree in the followong way: One iterates through the feature list and checks if a corresponding node/path exists in the FP-Tree. For nodes that already exist in the FP-Tree, one inrements the count of the node. The meaning of the node counter x is, that one has seen the pattern (encoded from that node upto the root of the tree) x times already. If while traversing the tree, one encounters the situation, where there exist no node for the corresponding feature, one adds a path from there on out for the remaining features and initializes those node counts to 1 (obvisouly, since we have seen the pattern only once). The cool thing is that one has encoded all information into this more compact tree structure. For my dataset this leads to a reduction factor of roughly 6, i.e. roughly 80% of the patterns fall onto eachother.

How one actually mines the patterns(features) efficiently from the FP-Tree and THEN ALSO stores them in such a way, that given a board, one can find the set of all frequent features quickly is a good question. Using the FP-Growth does not seem like a feasible option. Already for the FP-Tree I'm pretty much maxed out on memory (since I have to store the whole tree in memory). The FP-Growth algorithm would require to store all conditional FP-Trees in memory, which (atleast for my laptop) is not really possible without swapping. Additionaly the question remains how one can store the patterns in such a way, that given a board, one can find the set of all frequent features quickly.

Currently, the best idea I had was the follwing algorithm: I start at the root (null) node of the FP-Tree and add all its children to a candidate list (under the hood, the list is implemented as a priority queue). Now, I find the node (in the candidate list) with the highest count and traverse the path from this node up to the root node. Initially, this corresponds to the node itself, thus forming a pattern/feature of size 1. Next, I remove the node from the candidate list and add its children to the candidate list. I repeat steps N times to mine N patterns. What is happening here, is that I'm pruning the tree in a way, that I'm only including the N nodes, with highest count. It is worth noting that due to the structure of the FP-Tree, the mined patterns/features (the paths from leaves to the root) are not necessarily the N most frequent patterns/features.  

To find the patterns/features of a board, I can traverse the tree structure (which if quite efficient). The downside of this approach is that these are NOT the true N most frequent patterns, since subsets of the patterns are not considered. Additionaly there were frequent patterns in the original FP-Tree, which where distributed over multiple branches. These are not considered in the mined patterns. Especially for smaller patterns, this is a "big" problem, since, due to the structure of the FP-Tree, the only small patterns one finds, are those that dont occur with more frequent small patterns. I.e. the small patterns one finds are the moves that are played very late in the game (since otherwise it is likely there existed a more frequent pattern, which is a parent node of the small pattern). Many single and double pawn moves are mined, but queen, rook, etc. moves are not mined (against intuition), because they nearly always occur with other moves that are more frequent (like pawn moves), i.e. can be overshadowed by more frequent single patterns even though they are frequent in their own right.

## 24th June - 30th June